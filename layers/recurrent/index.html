<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <meta name="author" content="Aymeric Damien">
  <link rel="shortcut icon" href="../../img/favicon.ico">
  <title>Recurrent Layers - TFLearn</title>
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700' rel='stylesheet' type='text/css'>

  <link rel="stylesheet" href="../../css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../css/theme_extra.css" type="text/css" />
  <link rel="stylesheet" href="../../css/highlight.css">
  <link href="../../css/extra.css" rel="stylesheet">
  
  <script>
    // Current page data
    var mkdocs_page_name = "Recurrent Layers";
    var mkdocs_page_input_path = "layers/recurrent.md";
    var mkdocs_page_url = "/layers/recurrent/";
  </script>
  
  <script src="../../js/jquery-2.1.1.min.js"></script>
  <script src="../../js/modernizr-2.8.3.min.js"></script>
  <script type="text/javascript" src="../../js/highlight.pack.js"></script> 
  
  <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-81255389-1', 'tflearn.org');
      ga('send', 'pageview');
  </script>
  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
      <div class="wy-side-nav-search">
        <a href="../.." class="icon icon-home"> TFLearn</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
	<ul class="current">
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../..">Home</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../../doc_index/">Index</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../../installation/">Installation</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../../getting_started/">Getting Started</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../../tutorials/">Tutorials</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../../examples/">Examples</a>
        
    </li>
<li>
          
            <li>
    <ul class="subnav">
    <li><span>Models</span></li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../models/dnn/">Deep Neural Network</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../models/generator/">Generative Neural Network</a>
        
    </li>

        
    </ul>
<li>
          
            <li>
    <ul class="subnav">
    <li><span>Layers</span></li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../core/">Core Layers</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../conv/">Convolutional Layers</a>
        
    </li>

        
            
    <li class="toctree-l1 current">
        <a class="current" href="./">Recurrent Layers</a>
        
            <ul>
            
                <li class="toctree-l3"><a href="#simple-rnn">Simple RNN</a></li>
                
            
                <li class="toctree-l3"><a href="#lstm">LSTM</a></li>
                
            
                <li class="toctree-l3"><a href="#gru">GRU</a></li>
                
            
                <li class="toctree-l3"><a href="#bidirectional-rnn">Bidirectional RNN</a></li>
                
            
            </ul>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../normalization/">Normalization Layers</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../embedding_ops/">Embedding Layers</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../merge_ops/">Merge Layers</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../estimator/">Estimator Layers</a>
        
    </li>

        
    </ul>
<li>
          
            <li>
    <ul class="subnav">
    <li><span>Built-in Ops</span></li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../activations/">Activations</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../objectives/">Objectives</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../optimizers/">Optimizers</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../metrics/">Metrics</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../initializations/">Initializations</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../losses/">Losses</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../summaries/">Summaries</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../variables/">Variables</a>
        
    </li>

        
    </ul>
<li>
          
            <li>
    <ul class="subnav">
    <li><span>Data Management</span></li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../data_utils/">Data Utils</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../data_preprocessing/">Data Preprocessing</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../data_augmentation/">Data Augmentation</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../data_flow/">Data Flow</a>
        
    </li>

        
    </ul>
<li>
          
            <li>
    <ul class="subnav">
    <li><span>Others</span></li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../config/">Graph Config</a>
        
    </li>

        
    </ul>
<li>
          
            <li>
    <ul class="subnav">
    <li><span>Helpers for Extending Tensorflow</span></li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../helpers/trainer/">Trainer</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../helpers/evaluator/">Evaluator</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../helpers/summarizer/">Summarizer</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../helpers/regularizer/">Regularizer</a>
        
    </li>

        
    </ul>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../../contributions/">Contributions</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../../license/">License</a>
        
    </li>
<li>
          
        </ul>
      </div>
      &nbsp;
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="../..">TFLearn</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../..">Docs</a> &raquo;</li>
    
      
        
          <li>Layers &raquo;</li>
        
      
    
    <li>Recurrent Layers</li>
    <li class="wy-breadcrumbs-aside">
      
        <a href="https://github.com/tflearn/tflearn/edit/master/docs/layers/recurrent.md"
          class="icon icon-github"> Edit on GitHub</a>
      
    </li>
  </ul>
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <h1 id="simple-rnn">Simple RNN</h1>
<p><span class="extra_h1"><span style="color:black;"><b>tflearn.layers.recurrent.simple_rnn</b></span>  (incoming,  n_units,  activation='sigmoid',  dropout=None,  bias=True,  weights_init=None,  return_seq=False,  return_state=False,  initial_state=None,  dynamic=False,  trainable=True,  restore=True,  reuse=False,  scope=None,  name='SimpleRNN')</span></p>
<p>Simple Recurrent Layer.</p>
<h3>Input</h3>

<p>3-D Tensor [samples, timesteps, input dim].</p>
<h3>Output</h3>

<p>if <code>return_seq</code>: 3-D Tensor [samples, timesteps, output dim].
else: 2-D Tensor [samples, output dim].</p>
<h3>Arguments</h3>

<ul>
<li><strong>incoming</strong>: <code>Tensor</code>. Incoming 3-D Tensor.</li>
<li><strong>n_units</strong>: <code>int</code>, number of units for this layer.</li>
<li><strong>activation</strong>: <code>str</code> (name) or <code>function</code> (returning a <code>Tensor</code>).
Activation applied to this layer (see tflearn.activations).
Default: 'sigmoid'.</li>
<li><strong>dropout</strong>: <code>tuple</code> of <code>float</code>: (input_keep_prob, output_keep_prob). The
input and output keep probability.</li>
<li><strong>bias</strong>: <code>bool</code>. If True, a bias is used.</li>
<li><strong>weights_init</strong>: <code>str</code> (name) or <code>Tensor</code>. Weights initialization.
(See tflearn.initializations)</li>
<li><strong>return_seq</strong>: <code>bool</code>. If True, returns the full sequence instead of
last sequence output only.</li>
<li><strong>return_state</strong>: <code>bool</code>. If True, returns a tuple with output and
states: (output, states).</li>
<li><strong>initial_state</strong>: <code>Tensor</code>. An initial state for the RNN.  This must be
a tensor of appropriate type and shape [batch_size x cell.state_size].</li>
<li><strong>dynamic</strong>: <code>bool</code>. If True, dynamic computation is performed. It will not
compute RNN steps above the sequence length. Note that because TF
requires to feed sequences of same length, 0 is used as a mask.
So a sequence padded with 0 at the end must be provided. When
computation is performed, it will stop when it meets a step with
a value of 0.</li>
<li><strong>trainable</strong>: <code>bool</code>. If True, weights will be trainable.</li>
<li><strong>restore</strong>: <code>bool</code>. If True, this layer weights will be restored when
loading a model.</li>
<li><strong>reuse</strong>: <code>bool</code>. If True and 'scope' is provided, this layer variables
will be reused (shared).</li>
<li><strong>scope</strong>: <code>str</code>. Define this layer scope (optional). A scope can be
used to share variables between layers. Note that scope will
override name.</li>
<li><strong>name</strong>: <code>str</code>. A name for this layer (optional).</li>
</ul>
<hr />
<h1 id="lstm">LSTM</h1>
<p><span class="extra_h1"><span style="color:black;"><b>tflearn.layers.recurrent.lstm</b></span>  (incoming,  n_units,  activation='tanh',  inner_activation='sigmoid',  dropout=None,  bias=True,  weights_init=None,  forget_bias=1.0,  return_seq=False,  return_state=False,  initial_state=None,  dynamic=False,  trainable=True,  restore=True,  reuse=False,  scope=None,  name='LSTM')</span></p>
<p>Long Short Term Memory Recurrent Layer.</p>
<h3>Input</h3>

<p>3-D Tensor [samples, timesteps, input dim].</p>
<h3>Output</h3>

<p>if <code>return_seq</code>: 3-D Tensor [samples, timesteps, output dim].
else: 2-D Tensor [samples, output dim].</p>
<h3>Arguments</h3>

<ul>
<li><strong>incoming</strong>: <code>Tensor</code>. Incoming 3-D Tensor.</li>
<li><strong>n_units</strong>: <code>int</code>, number of units for this layer.</li>
<li><strong>activation</strong>: <code>str</code> (name) or <code>function</code> (returning a <code>Tensor</code>).
Activation applied to this layer (see tflearn.activations).
Default: 'tanh'.</li>
<li><strong>inner_activation</strong>: <code>str</code> (name) or <code>function</code> (returning a <code>Tensor</code>).
LSTM inner activation. Default: 'sigmoid'.</li>
<li><strong>dropout</strong>: <code>tuple</code> of <code>float</code>: (input_keep_prob, output_keep_prob). The
input and output keep probability.</li>
<li><strong>bias</strong>: <code>bool</code>. If True, a bias is used.</li>
<li><strong>weights_init</strong>: <code>str</code> (name) or <code>Tensor</code>. Weights initialization.
(See tflearn.initializations).</li>
<li><strong>forget_bias</strong>: <code>float</code>. Bias of the forget gate. Default: 1.0.</li>
<li><strong>return_seq</strong>: <code>bool</code>. If True, returns the full sequence instead of
last sequence output only.</li>
<li><strong>return_state</strong>: <code>bool</code>. If True, returns a tuple with output and
states: (output, states).</li>
<li><strong>initial_state</strong>: <code>Tensor</code>. An initial state for the RNN.  This must be
a tensor of appropriate type and shape [batch_size x cell.state_size].</li>
<li><strong>dynamic</strong>: <code>bool</code>. If True, dynamic computation is performed. It will not
compute RNN steps above the sequence length. Note that because TF
requires to feed sequences of same length, 0 is used as a mask.
So a sequence padded with 0 at the end must be provided. When
computation is performed, it will stop when it meets a step with
a value of 0.</li>
<li><strong>trainable</strong>: <code>bool</code>. If True, weights will be trainable.</li>
<li><strong>restore</strong>: <code>bool</code>. If True, this layer weights will be restored when
loading a model.</li>
<li><strong>reuse</strong>: <code>bool</code>. If True and 'scope' is provided, this layer variables
will be reused (shared).</li>
<li><strong>scope</strong>: <code>str</code>. Define this layer scope (optional). A scope can be
used to share variables between layers. Note that scope will
override name.</li>
<li><strong>name</strong>: <code>str</code>. A name for this layer (optional).</li>
</ul>
<h3>References</h3>

<p>Long Short Term Memory, Sepp Hochreiter &amp; Jurgen Schmidhuber,
Neural Computation 9(8): 1735-1780, 1997.</p>
<h3>Links</h3>

<p><a href="http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf">http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf</a></p>
<hr />
<h1 id="gru">GRU</h1>
<p><span class="extra_h1"><span style="color:black;"><b>tflearn.layers.recurrent.gru</b></span>  (incoming,  n_units,  activation='tanh',  inner_activation='sigmoid',  dropout=None,  bias=True,  weights_init=None,  return_seq=False,  return_state=False,  initial_state=None,  dynamic=False,  trainable=True,  restore=True,  reuse=False,  scope=None,  name='GRU')</span></p>
<p>Gated Recurrent Unit Layer.</p>
<h3>Input</h3>

<p>3-D Tensor Layer [samples, timesteps, input dim].</p>
<h3>Output</h3>

<p>if <code>return_seq</code>: 3-D Tensor [samples, timesteps, output dim].
else: 2-D Tensor [samples, output dim].</p>
<h3>Arguments</h3>

<ul>
<li><strong>incoming</strong>: <code>Tensor</code>. Incoming 3-D Tensor.</li>
<li><strong>n_units</strong>: <code>int</code>, number of units for this layer.</li>
<li><strong>activation</strong>: <code>str</code> (name) or <code>function</code> (returning a <code>Tensor</code>).
Activation applied to this layer (see tflearn.activations).
Default: 'tanh'.</li>
<li><strong>inner_activation</strong>: <code>str</code> (name) or <code>function</code> (returning a <code>Tensor</code>).
GRU inner activation. Default: 'sigmoid'.</li>
<li><strong>dropout</strong>: <code>tuple</code> of <code>float</code>: (input_keep_prob, output_keep_prob). The
input and output keep probability.</li>
<li><strong>bias</strong>: <code>bool</code>. If True, a bias is used.</li>
<li><strong>weights_init</strong>: <code>str</code> (name) or <code>Tensor</code>. Weights initialization.
(See tflearn.initializations).</li>
<li><strong>return_seq</strong>: <code>bool</code>. If True, returns the full sequence instead of
last sequence output only.</li>
<li><strong>return_state</strong>: <code>bool</code>. If True, returns a tuple with output and
states: (output, states).</li>
<li><strong>initial_state</strong>: <code>Tensor</code>. An initial state for the RNN.  This must be
a tensor of appropriate type and shape [batch_size x cell.state_size].</li>
<li><strong>dynamic</strong>: <code>bool</code>. If True, dynamic computation is performed. It will not
compute RNN steps above the sequence length. Note that because TF
requires to feed sequences of same length, 0 is used as a mask.
So a sequence padded with 0 at the end must be provided. When
computation is performed, it will stop when it meets a step with
a value of 0.</li>
<li><strong>trainable</strong>: <code>bool</code>. If True, weights will be trainable.</li>
<li><strong>restore</strong>: <code>bool</code>. If True, this layer weights will be restored when
loading a model.</li>
<li><strong>reuse</strong>: <code>bool</code>. If True and 'scope' is provided, this layer variables
will be reused (shared).</li>
<li><strong>scope</strong>: <code>str</code>. Define this layer scope (optional). A scope can be
used to share variables between layers. Note that scope will
override name.</li>
<li><strong>name</strong>: <code>str</code>. A name for this layer (optional).</li>
</ul>
<h3>References</h3>

<p>Learning Phrase Representations using RNN Encoderâ€“Decoder for
Statistical Machine Translation, K. Cho et al., 2014.</p>
<h3>Links</h3>

<p><a href="http://arxiv.org/abs/1406.1078">http://arxiv.org/abs/1406.1078</a></p>
<hr />
<h1 id="bidirectional-rnn">Bidirectional RNN</h1>
<p><span class="extra_h1"><span style="color:black;"><b>tflearn.layers.recurrent.bidirectional_rnn</b></span>  (incoming,  rnncell_fw,  rnncell_bw,  return_seq=False,  return_states=False,  initial_state_fw=None,  initial_state_bw=None,  dynamic=False,  scope=None,  name='BiRNN')</span></p>
<p>Build a bidirectional recurrent neural network, it requires 2 RNN Cells
to process sequence in forward and backward order. Any RNN Cell can be
used i.e. SimpleRNN, LSTM, GRU... with its own parameters. But the two
cells number of units must match.</p>
<h3>Input</h3>

<p>3-D Tensor Layer [samples, timesteps, input dim].</p>
<h3>Output</h3>

<p>if <code>return_seq</code>: 3-D Tensor [samples, timesteps, output dim].
else: 2-D Tensor Layer [samples, output dim].</p>
<h3>Arguments</h3>

<ul>
<li><strong>incoming</strong>: <code>Tensor</code>. The incoming Tensor.</li>
<li><strong>rnncell_fw</strong>: <code>RNNCell</code>. The RNN Cell to use for foward computation.</li>
<li><strong>rnncell_bw</strong>: <code>RNNCell</code>. The RNN Cell to use for backward computation.</li>
<li><strong>return_seq</strong>: <code>bool</code>. If True, returns the full sequence instead of
last sequence output only.</li>
<li><strong>return_states</strong>: <code>bool</code>. If True, returns a tuple with output and
states: (output, states).</li>
<li><strong>initial_state_fw</strong>: <code>Tensor</code>. An initial state for the forward RNN.
This must be a tensor of appropriate type and shape [batch_size
x cell.state_size].</li>
<li><strong>initial_state_bw</strong>: <code>Tensor</code>. An initial state for the backward RNN.
This must be a tensor of appropriate type and shape [batch_size
x cell.state_size].</li>
<li><strong>dynamic</strong>: <code>bool</code>. If True, dynamic computation is performed. It will not
compute RNN steps above the sequence length. Note that because TF
requires to feed sequences of same length, 0 is used as a mask.
So a sequence padded with 0 at the end must be provided. When
computation is performed, it will stop when it meets a step with
a value of 0.</li>
<li><strong>scope</strong>: <code>str</code>. Define this layer scope (optional). A scope can be
used to share variables between layers. Note that scope will
override name.</li>
<li><strong>name</strong>: <code>str</code>. A name for this layer (optional).</li>
</ul>
<hr />
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../normalization/" class="btn btn-neutral float-right" title="Normalization Layers">Next <span class="icon icon-circle-arrow-right"></span></a>
      
      
        <a href="../conv/" class="btn btn-neutral" title="Convolutional Layers"><span class="icon icon-circle-arrow-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
  </div>

  Built with <a href="http://www.mkdocs.org">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
	  
        </div>
      </div>

    </section>
    
  </div>

  <div class="rst-versions" role="note" style="cursor: pointer">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
          <a href="https://github.com/tflearn/tflearn" class="icon icon-github" style="float: left; color: #fcfcfc"> GitHub</a>
      
      
        <span><a href="../conv/" style="color: #fcfcfc;">&laquo; Previous</a></span>
      
      
        <span style="margin-left: 15px"><a href="../normalization/" style="color: #fcfcfc">Next &raquo;</a></span>
      
    </span>
</div>
    <script src="../../js/theme.js"></script>

</body>
</html>
