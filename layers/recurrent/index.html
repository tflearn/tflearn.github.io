<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <meta name="author" content="Aymeric Damien">
  
  <title>Recurrent Layers - TFLearn</title>
  

  <link rel="shortcut icon" href="../../img/favicon.ico">

  
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700' rel='stylesheet' type='text/css'>

  <link rel="stylesheet" href="../../css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../css/theme_extra.css" type="text/css" />
  <link rel="stylesheet" href="../../css/highlight.css">
  <link href="../../css/extra.css" rel="stylesheet">

  
  <script>
    // Current page data
    var mkdocs_page_name = "Recurrent Layers";
    var mkdocs_page_input_path = "layers/recurrent.md";
    var mkdocs_page_url = "/layers/recurrent/";
  </script>
  
  <script src="../../js/jquery-2.1.1.min.js"></script>
  <script src="../../js/modernizr-2.8.3.min.js"></script>
  <script type="text/javascript" src="../../js/highlight.pack.js"></script>
  <script src="../../js/theme.js"></script> 

  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
      <div class="wy-side-nav-search">
        <a href="../.." class="icon icon-home"> TFLearn</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
        <ul class="current">
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../..">Home</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../../doc_index/">Index</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../../installation/">Installation</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../../getting_started/">Getting Started</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../../examples/">Examples</a>
        
    </li>
<li>
          
            <li>
    <ul class="subnav">
    <li><span>Models</span></li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../models/dnn/">Deep Neural Network</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../models/generator/">Generative Neural Network</a>
        
    </li>

        
    </ul>
<li>
          
            <li>
    <ul class="subnav">
    <li><span>Layers</span></li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../core/">Core Layers</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../conv/">Convolutional Layers</a>
        
    </li>

        
            
    <li class="toctree-l1 current">
        <a class="current" href="./">Recurrent Layers</a>
        
            <ul>
            
                <li class="toctree-l3"><a href="#simple-rnn">Simple RNN</a></li>
                
            
                <li class="toctree-l3"><a href="#lstm">LSTM</a></li>
                
            
                <li class="toctree-l3"><a href="#gru">GRU</a></li>
                
            
                <li class="toctree-l3"><a href="#bidirectional-rnn">Bidirectional RNN</a></li>
                
            
                <li class="toctree-l3"><a href="#dynamic-rnn">Dynamic RNN</a></li>
                
            
            </ul>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../normalization/">Normalization Layers</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../embedding_ops/">Embedding Layers</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../merge_ops/">Merge Layers</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../estimator/">Estimator Layers</a>
        
    </li>

        
    </ul>
<li>
          
            <li>
    <ul class="subnav">
    <li><span>Built-in Ops</span></li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../activations/">Activations</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../objectives/">Objectives</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../optimizers/">Optimizers</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../metrics/">Metrics</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../initializations/">Initializations</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../losses/">Losses</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../summaries/">Summaries</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../variables/">Variables</a>
        
    </li>

        
    </ul>
<li>
          
            <li>
    <ul class="subnav">
    <li><span>Helpers for Extending Tensorflow</span></li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../helpers/trainer/">Trainer</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../helpers/evaluator/">Evaluator</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../helpers/summarizer/">Summarizer</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../helpers/regularizer/">Regularizer</a>
        
    </li>

        
    </ul>
<li>
          
            <li>
    <ul class="subnav">
    <li><span>Others</span></li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../data_preprocessing/">Data Preprocessing</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../data_augmentation/">Data Augmentation</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../data_flow/">Data Flow</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../data_utils/">Data Utils</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../config/">Graph Config</a>
        
    </li>

        
    </ul>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../../contributions/">Contributions</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../../license/">License</a>
        
    </li>
<li>
          
        </ul>
      </div>
      &nbsp;
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="../..">TFLearn</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../..">Docs</a> &raquo;</li>
    
      
        
          <li>Layers &raquo;</li>
        
      
    
    <li>Recurrent Layers</li>
    <li class="wy-breadcrumbs-aside">
      
        
          <a href="https://github.com/tflearn/tflearn" class="icon icon-github"> Edit on GitHub</a>
        
      
    </li>
  </ul>
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <h1 id="simple-rnn">Simple RNN</h1>
<p><span class="extra_h1"><span style="color:black;"><b>tflearn.layers.recurrent.simple_rnn</b></span>  (incoming,  n_units,  activation='sigmoid',  bias=True,  weights_init='truncated_normal',  return_seq=False,  return_states=False,  initial_state=None,  trainable=True,  restore=True,  name='SimpleRNN')</span></p>
<p>Simple Recurrent Layer.</p>
<h3>Input</h3>

<p>3-D Tensor [samples, timesteps, input dim].</p>
<h3>Output</h3>

<p>if <code>return_seq</code>: 3-D Tensor [samples, timesteps, output dim].
else: 2-D Tensor [samples, output dim].</p>
<h3>Arguments</h3>

<ul>
<li><strong>incoming</strong>: <code>Tensor</code>. Incoming 3-D Tensor.</li>
<li><strong>n_units</strong>: <code>int</code>, number of units for this layer.</li>
<li><strong>activation</strong>: <code>str</code> (name) or <code>Tensor</code>. Activation applied to this layer.
(See tflearn.activations). Default: 'sigmoid'.</li>
<li><strong>bias</strong>: <code>bool</code>. If True, a bias is used.</li>
<li><strong>weights_init</strong>: <code>str</code> (name) or <code>Tensor</code>. Weights initialization.
(See tflearn.initializations) Default: 'truncated_normal'.</li>
<li><strong>return_seq</strong>: <code>bool</code>. If True, returns the full sequence instead of
last sequence output only.</li>
<li><strong>return_states</strong>: <code>bool</code>. If True, returns a tuple with output and
states: (output, states).</li>
<li><strong>initial_state</strong>: <code>Tensor</code>. An initial state for the RNN.  This must be
a tensor of appropriate type and shape [batch_size x cell.state_size].</li>
<li><strong>name</strong>: <code>str</code>. A name for this layer (optional).</li>
</ul>
<hr />
<h1 id="lstm">LSTM</h1>
<p><span class="extra_h1"><span style="color:black;"><b>tflearn.layers.recurrent.lstm</b></span>  (incoming,  n_units,  activation='sigmoid',  inner_activation='tanh',  bias=True,  weights_init='truncated_normal',  forget_bias=1.0,  return_seq=False,  return_states=False,  initial_state=None,  trainable=True,  restore=True,  name='LSTM')</span></p>
<p>Long Short Term Memory Recurrent Layer.</p>
<h3>Input</h3>

<p>3-D Tensor [samples, timesteps, input dim].</p>
<h3>Output</h3>

<p>if <code>return_seq</code>: 3-D Tensor [samples, timesteps, output dim].
else: 2-D Tensor [samples, output dim].</p>
<h3>Arguments</h3>

<ul>
<li><strong>incoming</strong>: <code>Tensor</code>. Incoming 3-D Tensor.</li>
<li><strong>n_units</strong>: <code>int</code>, number of units for this layer.</li>
<li><strong>activation</strong>: <code>str</code> (name) or <code>Tensor</code>. Activation applied to this layer.
(See tflearn.activations). Default: 'sigmoid'.</li>
<li><strong>inner_activation</strong>: <code>str</code> (name) or <code>Tensor</code>. LSTM inner activation.
Default: 'tanh'.</li>
<li><strong>bias</strong>: <code>bool</code>. If True, a bias is used.</li>
<li><strong>weights_init</strong>: <code>str</code> (name) or <code>Tensor</code>. Weights initialization.
(See tflearn.initializations) Default: 'truncated_normal'.</li>
<li><strong>forget_bias</strong>: <code>float</code>. Bias of the forget gate. Default: 1.0.</li>
<li><strong>return_seq</strong>: <code>bool</code>. If True, returns the full sequence instead of
last sequence output only.</li>
<li><strong>return_states</strong>: <code>bool</code>. If True, returns a tuple with output and
states: (output, states).</li>
<li><strong>initial_state</strong>: <code>Tensor</code>. An initial state for the RNN.  This must be
a tensor of appropriate type and shape [batch_size x cell.state_size].</li>
<li><strong>name</strong>: <code>str</code>. A name for this layer (optional).</li>
</ul>
<h3>References</h3>

<p>Long Short Term Memory, Sepp Hochreiter &amp; Jurgen Schmidhuber,
Neural Computation 9(8): 1735-1780, 1997.</p>
<h3>Links</h3>

<p><a href="http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf">http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf</a></p>
<hr />
<h1 id="gru">GRU</h1>
<p><span class="extra_h1"><span style="color:black;"><b>tflearn.layers.recurrent.gru</b></span>  (incoming,  n_units,  activation='sigmoid',  inner_activation='tanh',  bias=True,  weights_init='truncated_normal',  return_seq=False,  return_states=False,  initial_state=None,  trainable=True,  restore=True,  name='GRU')</span></p>
<p>Gated Recurrent Unit Layer.</p>
<h3>Input</h3>

<p>3-D Tensor Layer [samples, timesteps, input dim].</p>
<h3>Output</h3>

<p>if <code>return_seq</code>: 3-D Tensor [samples, timesteps, output dim].
else: 2-D Tensor [samples, output dim].</p>
<h3>Arguments</h3>

<ul>
<li><strong>incoming</strong>: <code>Tensor</code>. Incoming 3-D Tensor.</li>
<li><strong>n_units</strong>: <code>int</code>, number of units for this layer.</li>
<li><strong>activation</strong>: <code>str</code> (name). Activation applied to this layer.
(See tflearn.activations). Default: 'sigmoid'.</li>
<li><strong>inner_activation</strong>: <code>str</code> (name) or <code>Tensor</code>. GRU inner activation.
Default: 'tanh'.</li>
<li><strong>bias</strong>: <code>bool</code>. If True, a bias is used.</li>
<li><strong>weights_init</strong>: <code>str</code> (name) or <code>Tensor</code>. Weights initialization.
(See tflearn.initializations) Default: 'truncated_normal'.</li>
<li><strong>return_seq</strong>: <code>bool</code>. If True, returns the full sequence instead of
last sequence output only.</li>
<li><strong>return_states</strong>: <code>bool</code>. If True, returns a tuple with output and
states: (output, states).</li>
<li><strong>initial_state</strong>: <code>Tensor</code>. An initial state for the RNN.  This must be
a tensor of appropriate type and shape [batch_size x cell.state_size].</li>
<li><strong>name</strong>: <code>str</code>. A name for this layer (optional).</li>
</ul>
<h3>References</h3>

<p>Learning Phrase Representations using RNN Encoderâ€“Decoder for
Statistical Machine Translation, K. Cho et al., 2014.</p>
<h3>Links</h3>

<p><a href="http://arxiv.org/abs/1406.1078">http://arxiv.org/abs/1406.1078</a></p>
<hr />
<h1 id="bidirectional-rnn">Bidirectional RNN</h1>
<p><span class="extra_h1"><span style="color:black;"><b>tflearn.layers.recurrent.bidirectional_rnn</b></span>  (incoming,  rnncell_fw,  rnncell_bw,  return_seq=False,  return_states=False,  initial_state_fw=None,  initial_state_bw=None,  name='BidirectionalRNN')</span></p>
<p>Build a bidirectional recurrent neural network, it requires 2 RNN Cells
to process sequence in forward and backward order. Any RNN Cell can be
used i.e. SimpleRNN, LSTM, GRU... with its own parameters. But the two
cells number of units must match.</p>
<h3>Input</h3>

<p>3-D Tensor Layer [samples, timesteps, input dim].</p>
<h3>Output</h3>

<p>if <code>return_seq</code>: 3-D Tensor [samples, timesteps, output dim].
else: 2-D Tensor Layer [samples, output dim].</p>
<h3>Arguments</h3>

<ul>
<li><strong>incoming</strong>: <code>Tensor</code>. The incoming Tensor.</li>
<li><strong>rnncell_fw</strong>: <code>RNNCell</code>. The RNN Cell to use for foward computation.</li>
<li><strong>rnncell_bw</strong>: <code>RNNCell</code>. The RNN Cell to use for backward computation.</li>
<li><strong>return_seq</strong>: <code>bool</code>. If True, returns the full sequence instead of
last sequence output only.</li>
<li><strong>return_states</strong>: <code>bool</code>. If True, returns a tuple with output and
states: (output, states).</li>
<li><strong>initial_state_fw</strong>: <code>Tensor</code>. An initial state for the forward RNN.
This must be a tensor of appropriate type and shape [batch_size
x cell.state_size].</li>
<li><strong>initial_state_bw</strong>: <code>Tensor</code>. An initial state for the backward RNN.
This must be a tensor of appropriate type and shape [batch_size
x cell.state_size].</li>
<li><strong>name</strong>: <code>str</code>. A name for this layer (optional).</li>
</ul>
<hr />
<h1 id="dynamic-rnn">Dynamic RNN</h1>
<p><span class="extra_h1"><span style="color:black;"><b>tflearn.layers.recurrent.dynamic_rnn</b></span>  (incoming,  rnncell,  sequence_length=None,  time_major=False,  return_seq=False,  return_states=False,  initial_state=None,  name='DynamicRNN')</span></p>
<p>RNN with dynamic sequence length.</p>
<p>Unlike <code>rnn</code>, the input <code>incoming</code> is not a Python list of <code>Tensors</code>.
Instead, it is a single <code>Tensor</code> where the maximum time is either the
first or second dimension (see the parameter <code>time_major</code>).  The
corresponding output is a single <code>Tensor</code> having the same number of time
steps and batch size.</p>
<p>The parameter <code>sequence_length</code> is required and dynamic calculation is
automatically performed.</p>
<h3>Input</h3>

<p>3-D Tensor Layer [samples, timesteps, input dim].</p>
<h3>Output</h3>

<p>if <code>return_seq</code>: 3-D Tensor [samples, timesteps, output dim].
else: 2-D Tensor Layer [samples, output dim].</p>
<h3>Arguments</h3>

<ul>
<li><strong>incoming</strong>: <code>Tensor</code>. The incoming 3-D Tensor.</li>
<li><strong>rnncell</strong>: <code>RNNCell</code>. The RNN Cell to use for computation.</li>
<li><strong>sequence_length</strong>: <code>int32</code> <code>Tensor</code>. A Tensor of shape [batch_size].
(Optional).</li>
<li><strong>time_major</strong>: The shape format of the <code>inputs</code> and <code>outputs</code> Tensors.
If true, these <code>Tensors</code> must be shaped <code>[max_time, batch_size, depth]</code>.
If false, these <code>Tensors</code> must be shaped <code>[batch_size, max_time, depth]</code>.
Using time_major = False is a bit more efficient because it avoids
transposes at the beginning and end of the RNN calculation.  However,
most TensorFlow data is batch-major, so by default this function
accepts input and emits output in batch-major form.</li>
<li><strong>return_seq</strong>: <code>bool</code>. If True, returns the full sequence instead of
last sequence output only.</li>
<li><strong>return_states</strong>: <code>bool</code>. If True, returns a tuple with output and
states: (output, states).</li>
<li><strong>initial_state</strong>: <code>Tensor</code>. An initial state for the RNN.  This must be
a tensor of appropriate type and shape [batch_size x cell.state_size].</li>
<li><strong>name</strong>: <code>str</code>. A name for this layer (optional).</li>
</ul>
<hr />
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../normalization/" class="btn btn-neutral float-right" title="Normalization Layers">Next <span class="icon icon-circle-arrow-right"></span></a>
      
      
        <a href="../conv/" class="btn btn-neutral" title="Convolutional Layers"><span class="icon icon-circle-arrow-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
  </div>

  Built with <a href="http://www.mkdocs.org">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
	  
        </div>
      </div>

    </section>

  </div>

<div class="rst-versions" role="note" style="cursor: pointer">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
          <a href="https://github.com/tflearn/tflearn" class="icon icon-github" style="float: left; color: #fcfcfc"> GitHub</a>
      
      
        <span><a href="../conv/" style="color: #fcfcfc;">&laquo; Previous</a></span>
      
      
        <span style="margin-left: 15px"><a href="../normalization/" style="color: #fcfcfc">Next &raquo;</a></span>
      
    </span>
</div>

</body>
</html>
